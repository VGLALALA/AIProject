{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "colab": {
   "provenance": [],
   "gpuType": "T4"
  },
  "kernelspec": {
   "name": "python3",
   "language": "python",
   "display_name": "Python 3 (ipykernel)"
  },
  "language_info": {
   "name": "python"
  },
  "accelerator": "GPU"
 },
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "8tgxs9pCUcfQ",
    "ExecuteTime": {
     "end_time": "2024-03-23T18:55:58.834936254Z",
     "start_time": "2024-03-23T18:55:58.800081051Z"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import matplotlib.pyplot as plt\n",
    "import torchvision\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets, transforms\n",
    "import numpy as np\n",
    "import random\n",
    "import os\n",
    "import glob"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Load Config"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Data Path: ./data/train\n",
      "Test Data Path: ./data/test\n",
      "Batch Size: 128\n",
      "Device: cuda\n",
      "Number of Epochs: 10\n",
      "Learning Rate: 0.0005\n"
     ]
    }
   ],
   "source": [
    "import configparser\n",
    "\n",
    "# Load the configuration file\n",
    "config = configparser.ConfigParser()\n",
    "config.read('cfg.ini')\n",
    "\n",
    "# Paths\n",
    "train_data_path = config['Paths']['TrainData']\n",
    "test_data_path = config['Paths']['TestData']\n",
    "\n",
    "# Data\n",
    "batch_size = int(config['Data']['BatchSize'])\n",
    "\n",
    "# Device\n",
    "device = config['Device']['device']\n",
    "\n",
    "# Pooling\n",
    "pool1_size = int(config['Pooling']['Pool1size'])\n",
    "pool1_stride = int(config['Pooling']['Pool1Stride'])\n",
    "pool2_size = int(config['Pooling']['Pool2size'])\n",
    "pool2_stride = int(config['Pooling']['Pool2Stride'])\n",
    "pool3_size = int(config['Pooling']['Pool3size'])\n",
    "pool3_stride = int(config['Pooling']['Pool3Stride'])\n",
    "\n",
    "# Convolutional Layers\n",
    "kernel_size1 = int(config['Conv']['KernelSize1'])\n",
    "kernel_size2 = int(config['Conv']['KernelSize2'])\n",
    "kernel_size3 = int(config['Conv']['KernelSize3'])\n",
    "kernel_size4 = int(config['Conv']['KernelSize4'])\n",
    "kernel_size5 = int(config['Conv']['KernelSize5'])\n",
    "conv1_in = int(config['Conv']['Conv1in'])\n",
    "conv1_out = int(config['Conv']['Conv1out'])\n",
    "conv2_in = int(config['Conv']['Conv2in'])\n",
    "conv2_out = int(config['Conv']['Conv2out'])\n",
    "conv3_in = int(config['Conv']['Conv3in'])\n",
    "conv3_out = int(config['Conv']['Conv3out'])\n",
    "conv4_in = int(config['Conv']['Conv4in'])\n",
    "conv4_out = int(config['Conv']['Conv4out'])\n",
    "conv5_in = int(config['Conv']['Conv5in'])\n",
    "conv5_out = int(config['Conv']['Conv5out'])\n",
    "\n",
    "# Linear Layers\n",
    "l1_in = int(config['Linear']['L1in'])\n",
    "l1_out = int(config['Linear']['L1out'])\n",
    "l2_in = int(config['Linear']['L2in'])\n",
    "l2_out = int(config['Linear']['L2out'])\n",
    "l3_in = int(config['Linear']['L3in'])\n",
    "l3_out = int(config['Linear']['L3out'])\n",
    "\n",
    "# View\n",
    "v1 = int(config['View']['V1'])\n",
    "v2 = int(config['View']['V2'])\n",
    "\n",
    "# Training\n",
    "num_epochs = int(config['Training']['NumEpochs'])\n",
    "learning_rate = float(config['Training']['LearningRate'])\n",
    "\n",
    "# Using the values\n",
    "print(f\"Training Data Path: {train_data_path}\")\n",
    "print(f\"Test Data Path: {test_data_path}\")\n",
    "print(f\"Batch Size: {batch_size}\")\n",
    "print(f\"Device: {device}\")\n",
    "print(f\"Number of Epochs: {num_epochs}\")\n",
    "print(f\"Learning Rate: {learning_rate}\")\n",
    "# Add more print statements as needed to display other configurations\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-23T18:55:58.860726617Z",
     "start_time": "2024-03-23T18:55:58.810361998Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Load The Dataset"
   ],
   "metadata": {
    "id": "VqyE74qeUxo9"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
    "])\n",
    "\n",
    "trainset = datasets.CIFAR10(root='./data', train=True, download=True, transform=transform)\n",
    "trainloader = DataLoader(trainset, batch_size=batch_size, shuffle=True, drop_last = True)\n",
    "\n",
    "testset = datasets.CIFAR10(root='./data', train=False, download=True, transform=transform)\n",
    "testloader = DataLoader(testset, batch_size=batch_size, shuffle=False, drop_last = True)"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "3VLSbmNbUgcN",
    "outputId": "3948bc0a-8e97-4b10-f854-7b92a9fc11f7",
    "ExecuteTime": {
     "end_time": "2024-03-23T18:56:06.008913599Z",
     "start_time": "2024-03-23T18:55:58.816003625Z"
    }
   },
   "execution_count": 16,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz to ./data/cifar-10-python.tar.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "57.7%IOPub message rate exceeded.\n",
      "The Jupyter server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--ServerApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "ServerApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "ServerApp.rate_limit_window=3.0 (secs)\n",
      "\n",
      "100.0%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./data/cifar-10-python.tar.gz to ./data\n",
      "Files already downloaded and verified\n"
     ]
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Design your own Network"
   ],
   "metadata": {
    "id": "H-DDO2TJaDpd"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# DLA"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DLA(\n",
      "  (base): Sequential(\n",
      "    (0): Conv2d(3, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "    (1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (2): ReLU(inplace=True)\n",
      "  )\n",
      "  (layer1): Sequential(\n",
      "    (0): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "    (1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (2): ReLU(inplace=True)\n",
      "  )\n",
      "  (layer2): Sequential(\n",
      "    (0): Conv2d(16, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "    (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (2): ReLU(inplace=True)\n",
      "  )\n",
      "  (layer3): Tree(\n",
      "    (root): Root(\n",
      "      (conv): Conv2d(128, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "    (left_node): BasicBlock(\n",
      "      (conv1): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (shortcut): Sequential(\n",
      "        (0): Conv2d(32, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (right_node): BasicBlock(\n",
      "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (shortcut): Sequential()\n",
      "    )\n",
      "  )\n",
      "  (layer4): Tree(\n",
      "    (root): Root(\n",
      "      (conv): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "    (level_1): Tree(\n",
      "      (root): Root(\n",
      "        (conv): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "      (left_node): BasicBlock(\n",
      "        (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (shortcut): Sequential(\n",
      "          (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "          (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        )\n",
      "      )\n",
      "      (right_node): BasicBlock(\n",
      "        (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (shortcut): Sequential()\n",
      "      )\n",
      "    )\n",
      "    (prev_root): BasicBlock(\n",
      "      (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (shortcut): Sequential(\n",
      "        (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (left_node): BasicBlock(\n",
      "      (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (shortcut): Sequential()\n",
      "    )\n",
      "    (right_node): BasicBlock(\n",
      "      (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (shortcut): Sequential()\n",
      "    )\n",
      "  )\n",
      "  (layer5): Tree(\n",
      "    (root): Root(\n",
      "      (conv): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "    (level_1): Tree(\n",
      "      (root): Root(\n",
      "        (conv): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "      (left_node): BasicBlock(\n",
      "        (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (shortcut): Sequential(\n",
      "          (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "          (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        )\n",
      "      )\n",
      "      (right_node): BasicBlock(\n",
      "        (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (shortcut): Sequential()\n",
      "      )\n",
      "    )\n",
      "    (prev_root): BasicBlock(\n",
      "      (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (shortcut): Sequential(\n",
      "        (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (left_node): BasicBlock(\n",
      "      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (shortcut): Sequential()\n",
      "    )\n",
      "    (right_node): BasicBlock(\n",
      "      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (shortcut): Sequential()\n",
      "    )\n",
      "  )\n",
      "  (layer6): Tree(\n",
      "    (root): Root(\n",
      "      (conv): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "    (left_node): BasicBlock(\n",
      "      (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (shortcut): Sequential(\n",
      "        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (right_node): BasicBlock(\n",
      "      (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (shortcut): Sequential()\n",
      "    )\n",
      "  )\n",
      "  (linear): Linear(in_features=512, out_features=10, bias=True)\n",
      ")\n",
      "torch.Size([1, 10])\n"
     ]
    }
   ],
   "source": [
    "class BasicBlock(nn.Module):\n",
    "    expansion = 1\n",
    "\n",
    "    def __init__(self, in_planes, planes, stride=1):\n",
    "        super(BasicBlock, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(\n",
    "            in_planes, planes, kernel_size=3, stride=stride, padding=1, bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(planes)\n",
    "        self.conv2 = nn.Conv2d(planes, planes, kernel_size=3,\n",
    "                               stride=1, padding=1, bias=False)\n",
    "        self.bn2 = nn.BatchNorm2d(planes)\n",
    "\n",
    "        self.shortcut = nn.Sequential()\n",
    "        if stride != 1 or in_planes != self.expansion*planes:\n",
    "            self.shortcut = nn.Sequential(\n",
    "                nn.Conv2d(in_planes, self.expansion*planes,\n",
    "                          kernel_size=1, stride=stride, bias=False),\n",
    "                nn.BatchNorm2d(self.expansion*planes)\n",
    "            )\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = F.relu(self.bn1(self.conv1(x)))\n",
    "        out = self.bn2(self.conv2(out))\n",
    "        out += self.shortcut(x)\n",
    "        out = F.relu(out)\n",
    "        return out\n",
    "\n",
    "\n",
    "class Root(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, kernel_size=1):\n",
    "        super(Root, self).__init__()\n",
    "        self.conv = nn.Conv2d(\n",
    "            in_channels, out_channels, kernel_size,\n",
    "            stride=1, padding=(kernel_size - 1) // 2, bias=False)\n",
    "        self.bn = nn.BatchNorm2d(out_channels)\n",
    "\n",
    "    def forward(self, xs):\n",
    "        x = torch.cat(xs, 1)\n",
    "        out = F.relu(self.bn(self.conv(x)))\n",
    "        return out\n",
    "\n",
    "\n",
    "class Tree(nn.Module):\n",
    "    def __init__(self, block, in_channels, out_channels, level=1, stride=1):\n",
    "        super(Tree, self).__init__()\n",
    "        self.level = level\n",
    "        if level == 1:\n",
    "            self.root = Root(2*out_channels, out_channels)\n",
    "            self.left_node = block(in_channels, out_channels, stride=stride)\n",
    "            self.right_node = block(out_channels, out_channels, stride=1)\n",
    "        else:\n",
    "            self.root = Root((level+2)*out_channels, out_channels)\n",
    "            for i in reversed(range(1, level)):\n",
    "                subtree = Tree(block, in_channels, out_channels,\n",
    "                               level=i, stride=stride)\n",
    "                self.__setattr__('level_%d' % i, subtree)\n",
    "            self.prev_root = block(in_channels, out_channels, stride=stride)\n",
    "            self.left_node = block(out_channels, out_channels, stride=1)\n",
    "            self.right_node = block(out_channels, out_channels, stride=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        xs = [self.prev_root(x)] if self.level > 1 else []\n",
    "        for i in reversed(range(1, self.level)):\n",
    "            level_i = self.__getattr__('level_%d' % i)\n",
    "            x = level_i(x)\n",
    "            xs.append(x)\n",
    "        x = self.left_node(x)\n",
    "        xs.append(x)\n",
    "        x = self.right_node(x)\n",
    "        xs.append(x)\n",
    "        out = self.root(xs)\n",
    "        return out\n",
    "\n",
    "\n",
    "class DLA(nn.Module):\n",
    "    def __init__(self, block=BasicBlock, num_classes=10):\n",
    "        super(DLA, self).__init__()\n",
    "        self.base = nn.Sequential(\n",
    "            nn.Conv2d(3, 16, kernel_size=3, stride=1, padding=1, bias=False),\n",
    "            nn.BatchNorm2d(16),\n",
    "            nn.ReLU(True)\n",
    "        )\n",
    "\n",
    "        self.layer1 = nn.Sequential(\n",
    "            nn.Conv2d(16, 16, kernel_size=3, stride=1, padding=1, bias=False),\n",
    "            nn.BatchNorm2d(16),\n",
    "            nn.ReLU(True)\n",
    "        )\n",
    "\n",
    "        self.layer2 = nn.Sequential(\n",
    "            nn.Conv2d(16, 32, kernel_size=3, stride=1, padding=1, bias=False),\n",
    "            nn.BatchNorm2d(32),\n",
    "            nn.ReLU(True)\n",
    "        )\n",
    "\n",
    "        self.layer3 = Tree(block,  32,  64, level=1, stride=1)\n",
    "        self.layer4 = Tree(block,  64, 128, level=2, stride=2)\n",
    "        self.layer5 = Tree(block, 128, 256, level=2, stride=2)\n",
    "        self.layer6 = Tree(block, 256, 512, level=1, stride=2)\n",
    "        self.linear = nn.Linear(512, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.base(x)\n",
    "        out = self.layer1(out)\n",
    "        out = self.layer2(out)\n",
    "        out = self.layer3(out)\n",
    "        out = self.layer4(out)\n",
    "        out = self.layer5(out)\n",
    "        out = self.layer6(out)\n",
    "        out = F.avg_pool2d(out, 4)\n",
    "        out = out.view(out.size(0), -1)\n",
    "        out = self.linear(out)\n",
    "        return out\n",
    "\n",
    "\n",
    "def test():\n",
    "    net = DLA()\n",
    "    print(net)\n",
    "    x = torch.randn(1, 3, 32, 32)\n",
    "    y = net(x)\n",
    "    print(y.size())\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    test()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-23T18:56:06.075166176Z",
     "start_time": "2024-03-23T18:56:06.013285777Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# ResNet"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "outputs": [],
   "source": [
    "# class BasicBlock(nn.Module):\n",
    "#     expansion = 1\n",
    "# \n",
    "#     def __init__(self, in_planes, planes, stride=1):\n",
    "#         super(BasicBlock, self).__init__()\n",
    "#         self.conv1 = nn.Conv2d(\n",
    "#             in_planes, planes, kernel_size=3, stride=stride, padding=1, bias=False)\n",
    "#         self.bn1 = nn.BatchNorm2d(planes)\n",
    "#         self.conv2 = nn.Conv2d(planes, planes, kernel_size=3,\n",
    "#                                stride=1, padding=1, bias=False)\n",
    "#         self.bn2 = nn.BatchNorm2d(planes)\n",
    "# \n",
    "#         self.shortcut = nn.Sequential()\n",
    "#         if stride != 1 or in_planes != self.expansion*planes:\n",
    "#             self.shortcut = nn.Sequential(\n",
    "#                 nn.Conv2d(in_planes, self.expansion*planes,\n",
    "#                           kernel_size=1, stride=stride, bias=False),\n",
    "#                 nn.BatchNorm2d(self.expansion*planes)\n",
    "#             )\n",
    "# \n",
    "#     def forward(self, x):\n",
    "#         out = F.relu(self.bn1(self.conv1(x)))\n",
    "#         out = self.bn2(self.conv2(out))\n",
    "#         out += self.shortcut(x)\n",
    "#         out = F.relu(out)\n",
    "#         return out\n",
    "# \n",
    "# \n",
    "# class Bottleneck(nn.Module):\n",
    "#     expansion = 4\n",
    "# \n",
    "#     def __init__(self, in_planes, planes, stride=1):\n",
    "#         super(Bottleneck, self).__init__()\n",
    "#         self.conv1 = nn.Conv2d(in_planes, planes, kernel_size=1, bias=False)\n",
    "#         self.bn1 = nn.BatchNorm2d(planes)\n",
    "#         self.conv2 = nn.Conv2d(planes, planes, kernel_size=3,\n",
    "#                                stride=stride, padding=1, bias=False)\n",
    "#         self.bn2 = nn.BatchNorm2d(planes)\n",
    "#         self.conv3 = nn.Conv2d(planes, self.expansion *\n",
    "#                                planes, kernel_size=1, bias=False)\n",
    "#         self.bn3 = nn.BatchNorm2d(self.expansion*planes)\n",
    "# \n",
    "#         self.shortcut = nn.Sequential()\n",
    "#         if stride != 1 or in_planes != self.expansion*planes:\n",
    "#             self.shortcut = nn.Sequential(\n",
    "#                 nn.Conv2d(in_planes, self.expansion*planes,\n",
    "#                           kernel_size=1, stride=stride, bias=False),\n",
    "#                 nn.BatchNorm2d(self.expansion*planes)\n",
    "#             )\n",
    "# \n",
    "#     def forward(self, x):\n",
    "#         out = F.relu(self.bn1(self.conv1(x)))\n",
    "#         out = F.relu(self.bn2(self.conv2(out)))\n",
    "#         out = self.bn3(self.conv3(out))\n",
    "#         out += self.shortcut(x)\n",
    "#         out = F.relu(out)\n",
    "#         return out\n",
    "# \n",
    "# \n",
    "# class ResNet(nn.Module):\n",
    "#     def __init__(self, block, num_blocks, num_classes=10):\n",
    "#         super(ResNet, self).__init__()\n",
    "#         self.in_planes = 64\n",
    "# \n",
    "#         self.conv1 = nn.Conv2d(3, 64, kernel_size=3,\n",
    "#                                stride=1, padding=1, bias=False)\n",
    "#         self.bn1 = nn.BatchNorm2d(64)\n",
    "#         self.layer1 = self._make_layer(block, 64, num_blocks[0], stride=1)\n",
    "#         self.layer2 = self._make_layer(block, 128, num_blocks[1], stride=2)\n",
    "#         self.layer3 = self._make_layer(block, 256, num_blocks[2], stride=2)\n",
    "#         self.layer4 = self._make_layer(block, 512, num_blocks[3], stride=2)\n",
    "#         self.linear = nn.Linear(512*block.expansion, num_classes)\n",
    "# \n",
    "#     def _make_layer(self, block, planes, num_blocks, stride):\n",
    "#         strides = [stride] + [1]*(num_blocks-1)\n",
    "#         layers = []\n",
    "#         for stride in strides:\n",
    "#             layers.append(block(self.in_planes, planes, stride))\n",
    "#             self.in_planes = planes * block.expansion\n",
    "#         return nn.Sequential(*layers)\n",
    "# \n",
    "#     def forward(self, x):\n",
    "#         out = F.relu(self.bn1(self.conv1(x)))\n",
    "#         out = self.layer1(out)\n",
    "#         out = self.layer2(out)\n",
    "#         out = self.layer3(out)\n",
    "#         out = self.layer4(out)\n",
    "#         out = F.avg_pool2d(out, 4)\n",
    "#         out = out.view(out.size(0), -1)\n",
    "#         out = self.linear(out)\n",
    "#         return out\n",
    "# \n",
    "# \n",
    "# def ResNet18():\n",
    "#     return ResNet(BasicBlock, [2, 2, 2, 2])\n",
    "# \n",
    "# \n",
    "# def ResNet34():\n",
    "#     return ResNet(BasicBlock, [3, 4, 6, 3])\n",
    "# \n",
    "# \n",
    "# def ResNet50():\n",
    "#     return ResNet(Bottleneck, [3, 4, 6, 3])\n",
    "# \n",
    "# \n",
    "# def ResNet101():\n",
    "#     return ResNet(Bottleneck, [3, 4, 23, 3])\n",
    "# \n",
    "# \n",
    "# def ResNet152():\n",
    "#     return ResNet(Bottleneck, [3, 8, 36, 3])\n",
    "# \n",
    "# \n",
    "# def test():\n",
    "#     net = ResNet18()\n",
    "#     y = net(torch.randn(1, 3, 32, 32))\n",
    "#     print(y.size())\n",
    "# \n",
    "# # test()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-23T18:56:06.078824100Z",
     "start_time": "2024-03-23T18:56:06.076572493Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Define Model\n"
   ],
   "metadata": {
    "id": "u5V1NnIzSbpw"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "class CustomNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CustomNN, self).__init__()\n",
    "        self.features = nn.Sequential(\n",
    "            nn.Conv2d(conv1_in, conv1_out, kernel_size = kernel_size1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(pool1_size, pool1_stride),\n",
    "            \n",
    "            nn.Conv2d(conv2_in, conv2_out, kernel_size = kernel_size2),\n",
    "            nn.ReLU(inplace=True),\n",
    "            \n",
    "            nn.Conv2d(conv3_in, conv3_out, kernel_size = kernel_size3),\n",
    "            nn.MaxPool2d(pool2_size, pool2_stride),\n",
    "            \n",
    "            nn.Conv2d(conv4_in, conv4_out, kernel_size = kernel_size4, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            \n",
    "            nn.Conv2d(conv5_in, conv5_out, kernel_size = kernel_size5, padding=1),\n",
    "            nn.MaxPool2d(kernel_size=pool3_size, stride=pool3_stride)\n",
    "            \n",
    "        )\n",
    "\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(l1_in * 6 * 6, l1_out),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Linear(l2_in, l2_out),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Linear(l3_in, l3_out)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.features(x)\n",
    "        x = x.view(v1, v2 * 6 * 6)\n",
    "        x = self.classifier(x)\n",
    "        \n",
    "        return x"
   ],
   "metadata": {
    "id": "bA5ZytDscalZ",
    "ExecuteTime": {
     "end_time": "2024-03-23T18:56:06.087054488Z",
     "start_time": "2024-03-23T18:56:06.079937475Z"
    }
   },
   "execution_count": 19,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Model Training\n"
   ],
   "metadata": {
    "id": "sN7PCSd6LMj5"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "model = DLA()\n",
    "model = model.to(device)"
   ],
   "metadata": {
    "id": "2aSByURldH5Q",
    "ExecuteTime": {
     "end_time": "2024-03-23T18:56:06.146747512Z",
     "start_time": "2024-03-23T18:56:06.081923363Z"
    }
   },
   "execution_count": 20,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "loss_history = []\n",
    "for epoch in range(num_epochs):\n",
    "    loss_per_epoch = 0\n",
    "    for images, label in trainloader:\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        images, label = images.to(device), label.to(device)\n",
    "\n",
    "        outputs = model(images)\n",
    "\n",
    "        loss = criterion(outputs, label)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        loss_per_epoch += loss\n",
    "\n",
    "    print(f\"epoch: {epoch + 1} / {num_epochs}, loss_per_epoch: {loss_per_epoch}\")\n",
    "    loss_history.append(loss_per_epoch.cpu().detach().numpy())\n",
    "\n",
    "# Question: Why I increase the batch size, the loss_per_epoch will decrease?\n",
    "# Write your answer here."
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "lJtCXqCidkCa",
    "outputId": "7e91b6d7-a20f-4848-c8a9-1826e4d08663",
    "ExecuteTime": {
     "end_time": "2024-03-23T18:56:13.938335850Z",
     "start_time": "2024-03-23T18:56:06.147791917Z"
    }
   },
   "execution_count": 21,
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[21], line 7\u001B[0m\n\u001B[1;32m      5\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m epoch \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mrange\u001B[39m(num_epochs):\n\u001B[1;32m      6\u001B[0m     loss_per_epoch \u001B[38;5;241m=\u001B[39m \u001B[38;5;241m0\u001B[39m\n\u001B[0;32m----> 7\u001B[0m     \u001B[38;5;28;01mfor\u001B[39;00m images, label \u001B[38;5;129;01min\u001B[39;00m trainloader:\n\u001B[1;32m      8\u001B[0m         optimizer\u001B[38;5;241m.\u001B[39mzero_grad()\n\u001B[1;32m     10\u001B[0m         images, label \u001B[38;5;241m=\u001B[39m images\u001B[38;5;241m.\u001B[39mto(device), label\u001B[38;5;241m.\u001B[39mto(device)\n",
      "File \u001B[0;32m~/anaconda3/envs/pythonProject/lib/python3.11/site-packages/torch/utils/data/dataloader.py:631\u001B[0m, in \u001B[0;36m_BaseDataLoaderIter.__next__\u001B[0;34m(self)\u001B[0m\n\u001B[1;32m    628\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_sampler_iter \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[1;32m    629\u001B[0m     \u001B[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001B[39;00m\n\u001B[1;32m    630\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_reset()  \u001B[38;5;66;03m# type: ignore[call-arg]\u001B[39;00m\n\u001B[0;32m--> 631\u001B[0m data \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_next_data\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    632\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_num_yielded \u001B[38;5;241m+\u001B[39m\u001B[38;5;241m=\u001B[39m \u001B[38;5;241m1\u001B[39m\n\u001B[1;32m    633\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_dataset_kind \u001B[38;5;241m==\u001B[39m _DatasetKind\u001B[38;5;241m.\u001B[39mIterable \u001B[38;5;129;01mand\u001B[39;00m \\\n\u001B[1;32m    634\u001B[0m         \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_IterableDataset_len_called \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;129;01mand\u001B[39;00m \\\n\u001B[1;32m    635\u001B[0m         \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_num_yielded \u001B[38;5;241m>\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_IterableDataset_len_called:\n",
      "File \u001B[0;32m~/anaconda3/envs/pythonProject/lib/python3.11/site-packages/torch/utils/data/dataloader.py:675\u001B[0m, in \u001B[0;36m_SingleProcessDataLoaderIter._next_data\u001B[0;34m(self)\u001B[0m\n\u001B[1;32m    673\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m_next_data\u001B[39m(\u001B[38;5;28mself\u001B[39m):\n\u001B[1;32m    674\u001B[0m     index \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_next_index()  \u001B[38;5;66;03m# may raise StopIteration\u001B[39;00m\n\u001B[0;32m--> 675\u001B[0m     data \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_dataset_fetcher\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mfetch\u001B[49m\u001B[43m(\u001B[49m\u001B[43mindex\u001B[49m\u001B[43m)\u001B[49m  \u001B[38;5;66;03m# may raise StopIteration\u001B[39;00m\n\u001B[1;32m    676\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_pin_memory:\n\u001B[1;32m    677\u001B[0m         data \u001B[38;5;241m=\u001B[39m _utils\u001B[38;5;241m.\u001B[39mpin_memory\u001B[38;5;241m.\u001B[39mpin_memory(data, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_pin_memory_device)\n",
      "File \u001B[0;32m~/anaconda3/envs/pythonProject/lib/python3.11/site-packages/torch/utils/data/_utils/fetch.py:51\u001B[0m, in \u001B[0;36m_MapDatasetFetcher.fetch\u001B[0;34m(self, possibly_batched_index)\u001B[0m\n\u001B[1;32m     49\u001B[0m         data \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdataset\u001B[38;5;241m.\u001B[39m__getitems__(possibly_batched_index)\n\u001B[1;32m     50\u001B[0m     \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m---> 51\u001B[0m         data \u001B[38;5;241m=\u001B[39m \u001B[43m[\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mdataset\u001B[49m\u001B[43m[\u001B[49m\u001B[43midx\u001B[49m\u001B[43m]\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43;01mfor\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[43midx\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;129;43;01min\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[43mpossibly_batched_index\u001B[49m\u001B[43m]\u001B[49m\n\u001B[1;32m     52\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m     53\u001B[0m     data \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdataset[possibly_batched_index]\n",
      "File \u001B[0;32m~/anaconda3/envs/pythonProject/lib/python3.11/site-packages/torch/utils/data/_utils/fetch.py:51\u001B[0m, in \u001B[0;36m<listcomp>\u001B[0;34m(.0)\u001B[0m\n\u001B[1;32m     49\u001B[0m         data \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdataset\u001B[38;5;241m.\u001B[39m__getitems__(possibly_batched_index)\n\u001B[1;32m     50\u001B[0m     \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m---> 51\u001B[0m         data \u001B[38;5;241m=\u001B[39m [\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mdataset\u001B[49m\u001B[43m[\u001B[49m\u001B[43midx\u001B[49m\u001B[43m]\u001B[49m \u001B[38;5;28;01mfor\u001B[39;00m idx \u001B[38;5;129;01min\u001B[39;00m possibly_batched_index]\n\u001B[1;32m     52\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m     53\u001B[0m     data \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdataset[possibly_batched_index]\n",
      "File \u001B[0;32m~/anaconda3/envs/pythonProject/lib/python3.11/site-packages/torchvision/datasets/cifar.py:115\u001B[0m, in \u001B[0;36mCIFAR10.__getitem__\u001B[0;34m(self, index)\u001B[0m\n\u001B[1;32m    111\u001B[0m img, target \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdata[index], \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mtargets[index]\n\u001B[1;32m    113\u001B[0m \u001B[38;5;66;03m# doing this so that it is consistent with all other datasets\u001B[39;00m\n\u001B[1;32m    114\u001B[0m \u001B[38;5;66;03m# to return a PIL Image\u001B[39;00m\n\u001B[0;32m--> 115\u001B[0m img \u001B[38;5;241m=\u001B[39m \u001B[43mImage\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mfromarray\u001B[49m\u001B[43m(\u001B[49m\u001B[43mimg\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    117\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mtransform \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[1;32m    118\u001B[0m     img \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mtransform(img)\n",
      "File \u001B[0;32m~/anaconda3/envs/pythonProject/lib/python3.11/site-packages/PIL/Image.py:3122\u001B[0m, in \u001B[0;36mfromarray\u001B[0;34m(obj, mode)\u001B[0m\n\u001B[1;32m   3119\u001B[0m     \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m   3120\u001B[0m         obj \u001B[38;5;241m=\u001B[39m obj\u001B[38;5;241m.\u001B[39mtostring()\n\u001B[0;32m-> 3122\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mfrombuffer\u001B[49m\u001B[43m(\u001B[49m\u001B[43mmode\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43msize\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mobj\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mraw\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mrawmode\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m0\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m1\u001B[39;49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/anaconda3/envs/pythonProject/lib/python3.11/site-packages/PIL/Image.py:3037\u001B[0m, in \u001B[0;36mfrombuffer\u001B[0;34m(mode, size, data, decoder_name, *args)\u001B[0m\n\u001B[1;32m   3034\u001B[0m         im\u001B[38;5;241m.\u001B[39mreadonly \u001B[38;5;241m=\u001B[39m \u001B[38;5;241m1\u001B[39m\n\u001B[1;32m   3035\u001B[0m         \u001B[38;5;28;01mreturn\u001B[39;00m im\n\u001B[0;32m-> 3037\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mfrombytes\u001B[49m\u001B[43m(\u001B[49m\u001B[43mmode\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43msize\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mdata\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mdecoder_name\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43margs\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/anaconda3/envs/pythonProject/lib/python3.11/site-packages/PIL/Image.py:2970\u001B[0m, in \u001B[0;36mfrombytes\u001B[0;34m(mode, size, data, decoder_name, *args)\u001B[0m\n\u001B[1;32m   2945\u001B[0m \u001B[38;5;250m\u001B[39m\u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[1;32m   2946\u001B[0m \u001B[38;5;124;03mCreates a copy of an image memory from pixel data in a buffer.\u001B[39;00m\n\u001B[1;32m   2947\u001B[0m \n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m   2965\u001B[0m \u001B[38;5;124;03m:returns: An :py:class:`~PIL.Image.Image` object.\u001B[39;00m\n\u001B[1;32m   2966\u001B[0m \u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[1;32m   2968\u001B[0m _check_size(size)\n\u001B[0;32m-> 2970\u001B[0m im \u001B[38;5;241m=\u001B[39m \u001B[43mnew\u001B[49m\u001B[43m(\u001B[49m\u001B[43mmode\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43msize\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   2971\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m im\u001B[38;5;241m.\u001B[39mwidth \u001B[38;5;241m!=\u001B[39m \u001B[38;5;241m0\u001B[39m \u001B[38;5;129;01mand\u001B[39;00m im\u001B[38;5;241m.\u001B[39mheight \u001B[38;5;241m!=\u001B[39m \u001B[38;5;241m0\u001B[39m:\n\u001B[1;32m   2972\u001B[0m     \u001B[38;5;66;03m# may pass tuple instead of argument list\u001B[39;00m\n\u001B[1;32m   2973\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mlen\u001B[39m(args) \u001B[38;5;241m==\u001B[39m \u001B[38;5;241m1\u001B[39m \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(args[\u001B[38;5;241m0\u001B[39m], \u001B[38;5;28mtuple\u001B[39m):\n",
      "File \u001B[0;32m~/anaconda3/envs/pythonProject/lib/python3.11/site-packages/PIL/Image.py:2941\u001B[0m, in \u001B[0;36mnew\u001B[0;34m(mode, size, color)\u001B[0m\n\u001B[1;32m   2939\u001B[0m     im\u001B[38;5;241m.\u001B[39mpalette \u001B[38;5;241m=\u001B[39m ImagePalette\u001B[38;5;241m.\u001B[39mImagePalette()\n\u001B[1;32m   2940\u001B[0m     color \u001B[38;5;241m=\u001B[39m im\u001B[38;5;241m.\u001B[39mpalette\u001B[38;5;241m.\u001B[39mgetcolor(color)\n\u001B[0;32m-> 2941\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m im\u001B[38;5;241m.\u001B[39m_new(\u001B[43mcore\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mfill\u001B[49m\u001B[43m(\u001B[49m\u001B[43mmode\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43msize\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mcolor\u001B[49m\u001B[43m)\u001B[49m)\n",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "plt.plot(list(range(num_epochs)), loss_history)\n",
    "plt.title(\"Traning Loss History\")\n",
    "plt.show()"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 452
    },
    "id": "ut9YA6nZOf2O",
    "outputId": "45ed25ac-b6b7-4f8b-99e3-85c1e8b6bab1",
    "ExecuteTime": {
     "end_time": "2024-03-23T18:56:13.940972602Z",
     "start_time": "2024-03-23T18:56:13.938570813Z"
    }
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Model Testing"
   ],
   "metadata": {
    "id": "iA9QleP7LPpO"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "# Test the model and show some predictions\n",
    "correct = 0\n",
    "total = 0\n",
    "\n",
    "with torch.no_grad():\n",
    "    for images, label in testloader:\n",
    "        images, label = images.to(device), label.to(device)\n",
    "        outputs = model(images)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += label.size(0)\n",
    "        correct += (predicted == label).sum().item()\n",
    "\n",
    "print(f\"Test Accuracy: {correct/total}\")"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ImeN9yeOdyX6",
    "outputId": "0742eaf3-eb34-4713-89ac-8d3d3c4222f7",
    "ExecuteTime": {
     "start_time": "2024-03-23T18:56:13.940184098Z"
    }
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "classes = ('plane', 'car', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck')"
   ],
   "metadata": {
    "id": "NATiftwSPOqp",
    "ExecuteTime": {
     "end_time": "2024-03-23T18:56:13.941721351Z",
     "start_time": "2024-03-23T18:56:13.941340384Z"
    }
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "images, labels = next(iter(testloader))\n",
    "images, labels = images.to(device), labels.to(device)\n",
    "\n",
    "# Predict\n",
    "with torch.no_grad():\n",
    "    outputs = model(images)\n",
    "    _, predicted = torch.max(outputs, 1)\n",
    "\n",
    "fig = plt.figure(figsize=(12, 6))\n",
    "\n",
    "for i in range(10):\n",
    "    ax = fig.add_subplot(2, 5, i + 1)\n",
    "    ax.imshow(images[i].cpu().numpy().transpose((1, 2, 0)) * 0.5 + 0.5)\n",
    "    ax.set_title('Pred: %s' % classes[predicted[i]])\n",
    "    ax.set_xticks([])\n",
    "    ax.set_yticks([])\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 560
    },
    "id": "rTc7SU5qPnfY",
    "outputId": "33416c75-ff1e-4e1d-e800-7d7378845c00",
    "ExecuteTime": {
     "start_time": "2024-03-23T18:56:13.942127405Z"
    }
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Logging"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import logging\n",
    "import configparser\n",
    "from datetime import datetime\n",
    "log_directory = '/home/vglalala/PycharmProjects/AIProject/log'\n",
    "# Set up basic configuration for logging\n",
    "logging.basicConfig(filename=f'{log_directory}/project_log_{datetime.now().strftime(\"%Y-%m-%d_%H-%M-%S\")}.log',\n",
    "                    level=logging.INFO,\n",
    "                    format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "\n",
    "# Function to log the configuration settings\n",
    "def log_config(config_path):\n",
    "    config = configparser.ConfigParser()\n",
    "    config.read(config_path)\n",
    "    for section in config.sections():\n",
    "        for key in config[section]:\n",
    "            logging.info(f\"CONFIG - {section} - {key}: {config[section][key]}\")\n",
    "\n",
    "# Example function to log training progress\n",
    "def log_training(epoch, loss_per_epoch):\n",
    "    \"\"\"Log the loss per epoch during training.\"\"\"\n",
    "    logging.info(f\"TRAINING - Epoch: {epoch}, Loss per Epoch: {loss_per_epoch}\")\n",
    "\n",
    "\n",
    "logging.info(f\"TEST - Accuracy: {correct/total}\")\n",
    "\n",
    "\n",
    "# Assuming you have a config file named 'settings.ini'\n",
    "log_config('cfg.ini')\n",
    "\n",
    "for i in range(1,num_epochs+1):\n",
    "    loss = loss_history[i-1]\n",
    "    log_training(i,loss)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2024-03-23T18:56:13.942917602Z"
    }
   }
  }
 ]
}
